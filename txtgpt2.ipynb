{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHnTCVUPJUjMp+7bS8lpqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amulya-B28/FutureIntern_AI_01/blob/main/txtgpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "0Dsa4olNmUGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzDiDqlXRLCf",
        "outputId": "fa00293b-0603-4c50-f123-0c532fa0b70e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX53ZPnURiGa",
        "outputId": "cc47ad76-1edc-428b-f7de-3590b5f1f8f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "u7l8FDmrmMTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# Add a padding token to the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load your custom dataset (or a dataset from the Hugging Face hub)\n",
        "dataset = load_dataset('Chadgpt-fam/sexting_dataset')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "OThsE91sSkQY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Dataset"
      ],
      "metadata": {
        "id": "EWrVjmWWmIn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# Add a padding token to the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Load your custom dataset (or a dataset from the Hugging Face hub)\n",
        "dataset = load_dataset('Therax/ADHD_data')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Output directory\n",
        "    num_train_epochs=1,              # Number of training epochs\n",
        "    per_device_train_batch_size=2,   # Batch size per device during training\n",
        "    per_device_eval_batch_size=2,    # Batch size for evaluation\n",
        "    warmup_steps=10,                # Warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # Strength of weight decay\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        ")\n",
        "\n",
        "# Define the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    # The dataset likely does not contain a validation split. Remove this or\n",
        "    # split the training set if you require a validation set.\n",
        "    # eval_dataset=tokenized_datasets['validation'],\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "# Set the labels to the input_ids\n",
        "#tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
        "trainer.train_dataset = tokenized_datasets[\"train\"].add_column(\"labels\", tokenized_datasets[\"train\"][\"input_ids\"])\n",
        "trainer.train(ignore_keys_for_eval=['past_key_values', 'hidden_states', 'attentions'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "rcTxUCy1TCoC",
        "outputId": "62100a57-fb1e-4463-bc31-1d7c6643913f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 12:22, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=15, training_loss=4.536001586914063, metrics={'train_runtime': 796.1488, 'train_samples_per_second': 0.038, 'train_steps_per_second': 0.019, 'total_flos': 15677521920000.0, 'train_loss': 4.536001586914063, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Dataset"
      ],
      "metadata": {
        "id": "GfOl4AtAmEd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_datasets = tokenized_datasets[\"train\"].train_test_split(test_size=0.2)\n",
        "results = trainer.evaluate(eval_dataset=split_datasets['test'])\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "iHBcIOCmTKyS",
        "outputId": "e0d961ae-caab-4b0b-d2d6-89bb4504b4d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:24]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_runtime': 36.9932, 'eval_samples_per_second': 0.162, 'eval_steps_per_second': 0.081, 'epoch': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random prompt"
      ],
      "metadata": {
        "id": "dJZFphQGl_dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_length=100, num_return_sequences=1)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_5uRpbrT78s",
        "outputId": "20f677ce-4958-4a3d-d959-47c0b6098918"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the world will be a better place.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWhsBHCJZwcz",
        "outputId": "83678239-c59d-4680-8d1d-f68fc571f20c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading GPT2 Model and Tokenizing Dataset"
      ],
      "metadata": {
        "id": "eMPdWeDplxXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the device (CPU or GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Tokenize the input from the dataset\n",
        "input_text = dataset['train'][0]['text']  # Sample from the dataset\n",
        "model_inputs = tokenizer(input_text, return_tensors='pt').to(device)\n"
      ],
      "metadata": {
        "id": "WYC4oeniaHXs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Greedy Search"
      ],
      "metadata": {
        "id": "aYEMhGu-kaCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new tokens based on the dataset input\n",
        "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
        "\n",
        "# Decode the generated tokens into human-readable text\n",
        "generated_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the output\n",
        "print(\"Generated Text:\\n\" + 100 * '-')\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iuJLOOXagwo",
        "outputId": "8080c860-3757-4336-cfbd-6dda7f8c4a6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "I've been feeling this way for a long time. I've been feeling this way for a long time. I've been feeling this way for a long time. I've been feeling this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Beam search"
      ],
      "metadata": {
        "id": "n4SzSXYtkc32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate beam search and early stopping\n",
        "beam_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,     # Generate 40 new tokens\n",
        "    num_beams=5,           # Use 5 beams for beam search\n",
        "    early_stopping=True    # Stop early when the model finds a good output\n",
        ")\n",
        "\n",
        "# Decode the generated tokens into human-readable text\n",
        "generated_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the output\n",
        "print(\"Generated Text:\\n\" + 100 * '-')\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "YqTxwQLhao88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7c6255-0dcf-423e-ffe5-f365c31a45ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "I've been feeling this way for a long time. I've been feeling this way for a long time. I've been feeling this way for a long time. I've been feeling this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Beam search with no repeated n-grams"
      ],
      "metadata": {
        "id": "3kNdiMQZkqkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate\n",
        "beam_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,          # Generate 40 new tokens\n",
        "    num_beams=5,                # Use 5 beams for beam search\n",
        "    no_repeat_ngram_size=2,     # Prevent repetition of 2-grams (two-word phrases)\n",
        "    early_stopping=True         # Stop early when the model finds a good output\n",
        ")\n",
        "\n",
        "# Decode the generated tokens into human-readable text\n",
        "generated_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the output\n",
        "print(\"Generated Text:\\n\" + 100 * '-')\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szpB6AhBbAyf",
        "outputId": "1622e02b-da5a-40a0-f9a0-a584b5e4d916"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "I've had a lot of anxiety and panic attacks in the past few years, but I've never felt that way about anything. It's just that I don't know how to deal with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate 5 sequences using beam search with no repeated n-grams"
      ],
      "metadata": {
        "id": "Gv0FbAAQkwp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beam_outputs = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=60,          # Generate 40 new tokens\n",
        "    num_beams=5,                # Use 5 beams for beam search\n",
        "    no_repeat_ngram_size=2,     # Prevent repetition of 2-grams\n",
        "    num_return_sequences=5,     # Generate and return 5 sequences\n",
        "    early_stopping=True         # Stop early when a good output is found\n",
        ")\n",
        "\n",
        "# Now we have 5 output sequences\n",
        "print(\"Generated Text:\\n\" + 100 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "    print(f\"Output {i + 1}:\")\n",
        "    print(tokenizer.decode(beam_output, skip_special_tokens=True))\n",
        "    print(100 * '-')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T3Dcan9bWcx",
        "outputId": "c2b77e4a-0243-4fec-dec2-acbc9d6d96fc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Output 1:\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "I've had a lot of anxiety and panic attacks in the past few years, but I've never felt that way about anything. It's just that I don't know how to deal with it. I think it's because I'm so focused on my job and my family, and I\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Output 2:\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "I've had a lot of anxiety and panic attacks in the past few years, but I've never felt that way about anything. It's just that I don't know how to deal with it. I'm not sure if it's because of the stress, or if I just have a\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Output 3:\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "I've had a lot of anxiety and panic attacks in the past few years, but I've never felt that way about anything. It's just that I don't know how to deal with it. I think it's because I'm so focused on my job and my family, and that\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Output 4:\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "I've had a lot of anxiety and panic attacks in the past few years, but I've never felt that way about anything. It's just that I don't know how to deal with it. I'm not sure if it's because of the stress, or if I just feel like\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Output 5:\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "I've had a lot of anxiety and panic attacks in the past few years, but I've never felt that way about anything. It's just that I don't know how to deal with it. I think it's because I'm so focused on my job and my family, and it\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activate Sampling"
      ],
      "metadata": {
        "id": "A4PzSwZ7k2Wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,     # Generate 40 new tokens\n",
        "    do_sample=True,        # Enable sampling for more diverse outputs\n",
        "    top_k=0                # Allow sampling from the full distribution (no top-k restriction)\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(\"Generated Text:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeHdjkADbttE",
        "outputId": "f9f35ce4-6bb3-41a4-c359-0ad9b5e0b3c0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things? What's the barbell-chest feeling like at night... Or at home, or apart from the workout or office routines that have inspired your TV habits?\n",
            "\n",
            "How often have you discovered that adding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use temperature to decrease the sensitivity to low probability candidates\n"
      ],
      "metadata": {
        "id": "UqKFpdQ0k6zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=0,\n",
        "    temperature=0.6,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNa6sJ4hcTTx",
        "outputId": "2cc5100b-3010-4ac0-95da-f7463778cc8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things? In the U.S., about 30% of Americans report having a reaction to something as \"disordered.\" According to a recent survey of 5,000 U.S. adults, about half of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top-K Sampling"
      ],
      "metadata": {
        "id": "zKPGVZ_TlWXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDM1q4gQcdX6",
        "outputId": "43fc19df-b7d2-4591-9116-e49ef6767ca1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "It is usually one of those things. If you are stressed in your daily life and nervous, that is not healthy. And you are in extreme risk for panic attacks because your body is not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top-p (nucleus) sampling"
      ],
      "metadata": {
        "id": "v0MULw6kldob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2crcv5lcqA_",
        "outputId": "2e822e51-34d7-4712-e67a-20707c4790fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "Oh, and even when it feels like there's no point playing basketball, you always give me a break. I need other people to bring me new things. To bring me new games.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top P-Sampling"
      ],
      "metadata": {
        "id": "I0fSa1AAlgNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUlQjkSmcvhE",
        "outputId": "24203d51-af99-449c-8528-083efb295a1a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: How often have you been feeling excessive worry or nervousness, even about everyday things? It is because I try to make myself seem confident, healthy, and fit. Unfortunately, I have experienced these things quite often. I had to deal with an almost unbearable and scary feeling. I also\n",
            "1: How often have you been feeling excessive worry or nervousness, even about everyday things?\n",
            "\n",
            "You've probably experienced these. Maybe you're just a little afraid of the stress and worry it's going to get to you.\n",
            "\n",
            "Maybe you've made yourself more and more anxious,\n",
            "2: How often have you been feeling excessive worry or nervousness, even about everyday things? That's the big question facing me now—why do you think I'm experiencing it all so quickly?\n",
            "\n",
            "Why do you think it has happened so fast?\n",
            "\n",
            "Why does it matter that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtxSojVcc192"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}